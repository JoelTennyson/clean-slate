---
output: pdf_document
---
We're looking to expand our proxy-state analysis to additional years, in order to make sure we're comfortable using PA as a proxy for MA crime. Here we're combining data from 2014-2018 (from what was usually known as table 69, aggregated arrest rates, save one year it was reported as table 2) and repeating our look at euclidian distance vs MA with both standardized and non-standardized variables.

So first things first, let's load our libraries and data:
```{r}
knitr::opts_chunk$set(warning = F)
library(here) ## relative pathways
library(readxl) ## reading excel
library(dplyr) ## data manipulation
library(tidyr) ## nesting dataframes
library(purrr) ## map reduce functions
FBIData <- read_excel(here("data", "cleaned", "fbi_aggregated_data_combined/FBI_aggregate_crime_data_2014_2018.xlsx"))
```

Now we're repeating the same analysis done in the other FBI folder here, but nesting across state and year (so we'll compare each each state in 2018 to MA 2018). 
```{r}
nestedFBIData <- FBIData %>%
  filter(age_category == "Under 18") %>%
  ## these variables are the same ones done with 2014 data
  select(state, 
         year, 
         robbery, 
         property_crime, 
         burglary, 
         larceny_theft, 
         motor_vehicle_theft, 
         estimated_population) %>%
  mutate(year = as.character(year)) %>%
  ## get per-capita crime rate
  mutate_if(is.numeric, funs(. / estimated_population)) %>%
  select(-estimated_population) %>%
  ## this bit is wonky if you don't know R; I'm creating a column of dataframes
  ## containing data for only that state
  nest(data = -c(state, year)) %>%
  rename(other_states = data)

## this is decidedly not the smartest way to do this, but:
## iterate across each state, combining its dataframe with the MA one;
## create a list of those dataframes in "frames"
frames <- vector("list")
for (i in seq_along(nestedFBIData$state)) {
  yr <- nestedFBIData[[i, "year"]]
  st <- nestedFBIData[[i, "state"]]
  frames[i][1] <- nest(data = everything(), 
                    rbind(nestedFBIData[[i, "other_states"]], 
                          nestedFBIData[nestedFBIData$state == "Massachusetts" & nestedFBIData$year == yr, ][[3]][[1]]))
  
}

## now iterate through those dataframes calulating euclidian distance
## (same metric as last time)
distScore <- vector()
for (i in seq_along(frames)) {
  distScore[i] <- dist(frames[[i]][[1]])
}
## now let's label those scores and examine the results!
distScores <- as_tibble(cbind(nestedFBIData$state, nestedFBIData$year, distScore)) %>%
  arrange(distScore) %>%
  rename(State = V1,
         Year = V2)
FBIData %>%
  filter(state == "Pennsylvania" & 
           year == 2018) 
```

So PA in this quick experiment comes in 10th place (if you count MA as 1st; which is... a choice) -- which is still 80th percentile, but I'm wondering if we had pragmatic considerations for selecting PA in addition to distance-based ones. I think that, unless it turns out that New Jersey publishes all their crimes in a Google Spreadsheet emailed to everyone on New Year's, we can still justify looking at PA with this result -- the distinction between any top 10 state other than NJ and maybe Kentucky is extremely minimal.

Of course, the last analysis standardized the variables we looked at as well, so that rates of offense among less common crimes could be weighted equally to more common offenses. I need to do an inch more reading to have an opinion on this, I think -- it seems to me like we should care about the crimes with more absolute cases more here (and weight them accordingly), as our desired outcome isn't as much "what state has the same offender profiles" as it is "what has the same rate of crimes" -- that is, I think that a large percentage difference in a more common crime category is more important to us than a similar magnitude but smaller absolute number difference in a less common one. But I'm not entirely sure, so here's the analysis run with standardized data:
```{r}
nestedFBIData <- FBIData %>%
  filter(age_category == "Under 18" &
    state != "Iowa") %>%
  select(state, 
         year, 
         robbery, 
         property_crime, 
         burglary, 
         larceny_theft, 
         motor_vehicle_theft, 
         estimated_population) %>%
  mutate_if(is.numeric, funs(. / estimated_population)) %>%
  mutate_if(is.numeric, scale) %>%
  select(-estimated_population) %>%
  nest(data = -c(state)) %>%
  rename(other_states = data)

frames <- vector("list")
for (i in seq_along(nestedFBIData$state)) {
  frames[i] <- nest(data = everything(), 
                    rbind(nestedFBIData[[i, "other_states"]], 
                          nestedFBIData[nestedFBIData$state == "Massachusetts", ][[2]][[1]]))
}

## now iterate through those dataframes calulating euclidian distance
## (same metric as last time)
distScoreStand <- vector()
for (i in seq_along(frames)) {
  distScoreStand[i] <- dist(frames[[i]][[1]])
}
## now let's label those scores and examine the results!
standScores <- as_tibble(cbind(nestedFBIData$state, distScoreStand)) %>%
  arrange(distScoreStand) %>%
  rename(State = V1)

standScores

standScores %>%
  tail(-10)
```

PA now is number 12, down two spots. Notably, Vermont came from almost dead last to an undisputable second place here. Having Vermont and New Hampshire here makes an amount of sense to me -- those are extremely similar states, after all. I'll look forward to talking about this with people on Tuesday -- I'm not even clear myself on what this implies for our next steps.

```{r}
library(ggplot2)
graphFrame <- distScores %>%
  full_join(standScores) %>%
  drop_na()

graphFrame %>%
  mutate(abb = setNames(state.abb, state.name)[State]) %>%
  ggplot(aes(distScore, distScoreStand)) + 
  geom_text(aes(label = abb))
```
